{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469670a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package loaded successfully.\n",
      "Raw data file opened successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load packages and Files\n",
    "\n",
    "from pyhipp.io import h5\n",
    "import numpy as np\n",
    "import argparse\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from pyhipp_sims.sims.sim_info import SimInfo\n",
    "from scipy.fft import rfftn, irfftn\n",
    "\n",
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "print(\"Package loaded successfully.\")\n",
    "\n",
    "BASE_PATH = Path('/data14/Hao_L/TNG100-1-Dark')\n",
    "OUTPUT_PATH = Path('/data14/Fhong/halo_web')\n",
    "CONFIG_FILE = OUTPUT_PATH / 'tng100-1-dark.json'\n",
    "PREPROCESSED_FILE = OUTPUT_PATH / 'tng100-1-dark_postprocessed.hdf5'\n",
    "\n",
    "num_processes = 8\n",
    "\n",
    "data = h5py.File(PREPROCESSED_FILE, \"r\", locking=False)\n",
    "if data:\n",
    "    print(\"Raw data file opened successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0271a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Selecting Final Halo Sample ---\n",
      "Loading simulation config from: /data14/Fhong/halo_web/tng100-1-dark.json\n",
      "Loading pre-processed catalog from: /data14/Fhong/halo_web/tng100-1-dark_postprocessed.hdf5\n",
      "Applying final selection criteria...\n",
      "Final selection: 23653 halos match the paper's criteria.\n",
      "Converting 'snap_form' to 'zf' and preparing final dataset...\n",
      "\n",
      "Final halo sample for analysis saved to: /data14/Fhong/halo_web/halo_sample.hdf5\n",
      "Script finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# Generate Halo Samples\n",
    "\n",
    "MIN_LOG_MASS = 10.5\n",
    "MAX_LOG_MASS = 11.0\n",
    "EXCLUDE_BACKSPLASH = True\n",
    "\n",
    "OUTPUT_FILE = OUTPUT_PATH / 'halo_sample.hdf5'\n",
    "\n",
    "print(\"--- Selecting Final Halo Sample ---\")\n",
    "\n",
    "# --- Loading File ---\n",
    "print(f\"Loading simulation config from: {CONFIG_FILE}\")\n",
    "config_file_path = Path(CONFIG_FILE)\n",
    "if not config_file_path.exists():\n",
    "    print(f\"FATAL ERROR: Configuration file not found at '{config_file_path}'\")\n",
    "    \n",
    "with open(config_file_path, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "\n",
    "sim_info = SimInfo(\n",
    "    name=config_data['name'],\n",
    "    root_dir=Path(BASE_PATH),\n",
    "    root_file=Path(config_data['root_file']),\n",
    "    sim_def=config_data['simulation_definition']\n",
    ")\n",
    "\n",
    "input_file_path = Path(PREPROCESSED_FILE)\n",
    "if not input_file_path.exists():\n",
    "    print(f\"FATAL ERROR: Pre-processed file not found at '{input_file_path}'\")\n",
    "    print(\"Please run Stage 1 script first.\")\n",
    "    \n",
    "print(f\"Loading pre-processed catalog from: {input_file_path}\")\n",
    "with h5py.File(input_file_path, 'r', locking=False) as f_in:\n",
    "    props = {key: val[:] for key, val in f_in['properties'].items()}\n",
    "    h = 0.6774\n",
    "\n",
    "# --- Halo Selection ---\n",
    "print(\"Applying final selection criteria...\")\n",
    "\n",
    "mass_Msun = props['m_mean200'] * 1e10 / h\n",
    "log_mass = np.log10(mass_Msun, where=mass_Msun > 0)\n",
    "mass_mask = (log_mass >= MIN_LOG_MASS) & (log_mass < MAX_LOG_MASS)\n",
    "\n",
    "final_mask = mass_mask\n",
    "if EXCLUDE_BACKSPLASH:\n",
    "    backsplash_mask = props['last_sat_snap'] == -1\n",
    "    final_mask &= backsplash_mask\n",
    "    \n",
    "final_indices = np.where(final_mask)[0]\n",
    "\n",
    "if len(final_indices) == 0:\n",
    "    print(\"\\nError: No halos found matching all criteria.\")\n",
    "    print(f\"   - Found {np.sum(mass_mask)} halos in the mass range.\")\n",
    "    if EXCLUDE_BACKSPLASH:\n",
    "        print(f\"   - Of those, {np.sum(mass_mask & backsplash_mask)} also passed the backsplash cut.\")\n",
    "    \n",
    "print(f\"Final selection: {len(final_indices)} halos match the paper's criteria.\")\n",
    "\n",
    "# --- snap_form -> zf ---\n",
    "print(\"Converting 'snap_form' to 'zf' and preparing final dataset...\")\n",
    "\n",
    "snap_form = props['snap_form'][final_indices].astype(int)\n",
    "\n",
    "valid_snaps_mask = (snap_form >= 0) & (snap_form < len(sim_info.redshifts))\n",
    "\n",
    "zf = np.full_like(snap_form, -1.0, dtype=np.float32)\n",
    "\n",
    "valid_snap_indices = snap_form[valid_snaps_mask]\n",
    "zf[valid_snaps_mask] = sim_info.redshifts[valid_snap_indices]\n",
    "\n",
    "final_data = {\n",
    "    'mass': props['m_crit200'][final_indices] * 1e10 / h,\n",
    "    'zf': zf,\n",
    "    'position': props['x'][final_indices],\n",
    "    'm_peak': props['m_peak'][final_indices] * 1e10 / h,\n",
    "    'v_peak': props['v_peak'][final_indices],\n",
    "    'spin': props['spin'][final_indices]\n",
    "}\n",
    "\n",
    "output_file = Path(OUTPUT_FILE)\n",
    "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with h5py.File(output_file, 'w', locking=False) as f_out:\n",
    "    f_out.attrs['simulation'] = sim_info.formal_name or sim_info.name\n",
    "    f_out.attrs['n_halos'] = len(final_indices)\n",
    "    f_out.attrs['mass_range_logMsun'] = [MIN_LOG_MASS, MAX_LOG_MASS]\n",
    "    f_out.attrs['backsplash_excluded'] = EXCLUDE_BACKSPLASH\n",
    "    \n",
    "    for key, data in final_data.items():\n",
    "        f_out.create_dataset(key, data=data)\n",
    "        \n",
    "print(f\"\\nFinal halo sample for analysis saved to: {output_file}\")\n",
    "print(\"Script finished successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a5b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading header from: /data14/Hao_L/TNG100-1-Dark/output/snapdir_099/snap_099.0.hdf5\n",
      "\n",
      "Starting parallel density field calculation with 8 processes...\n",
      "[Worker 20442] Processing chunk 4...[Worker 20443] Processing chunk 6...[Worker 20441] Processing chunk 2...[Worker 20444] Processing chunk 8...[Worker 20445] Processing chunk 10...[Worker 20446] Processing chunk 12...[Worker 20440] Processing chunk 0...\n",
      "\n",
      "\n",
      "[Worker 20447] Processing chunk 14...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Worker 20440] Finished chunk 0, found 94082254 particles.\n",
      "[Worker 20440] Processing chunk 1...\n",
      "[Worker 20447] Finished chunk 14, found 94099645 particles.\n",
      "[Worker 20447] Processing chunk 15...\n",
      "[Worker 20441] Finished chunk 2, found 94168133 particles.\n",
      "[Worker 20441] Processing chunk 3...\n",
      "[Worker 20442] Finished chunk 4, found 93802639 particles.\n",
      "[Worker 20442] Processing chunk 5...\n",
      "[Worker 20446] Finished chunk 12, found 94152614 particles.\n",
      "[Worker 20443] Finished chunk 6, found 94467000 particles.\n",
      "[Worker 20446] Processing chunk 13...\n",
      "[Worker 20443] Processing chunk 7...\n",
      "[Worker 20444] Finished chunk 8, found 93995753 particles.\n",
      "[Worker 20444] Processing chunk 9...\n",
      "[Worker 20445] Finished chunk 10, found 94125533 particles.\n",
      "[Worker 20445] Processing chunk 11...\n",
      "[Worker 20447] Finished chunk 15, found 94319916 particles.\n",
      "[Worker 20447] Processing chunk 16...\n",
      "[Worker 20440] Finished chunk 1, found 93716342 particles.\n",
      "[Worker 20440] Processing chunk 18...\n",
      "[Worker 20441] Finished chunk 3, found 94015230 particles.\n",
      "[Worker 20441] Processing chunk 20...\n",
      "[Worker 20442] Finished chunk 5, found 94091358 particles.\n",
      "[Worker 20442] Processing chunk 22...\n",
      "[Worker 20444] Finished chunk 9, found 94434733 particles.\n",
      "[Worker 20444] Processing chunk 24...\n",
      "[Worker 20443] Finished chunk 7, found 93840105 particles.\n",
      "[Worker 20446] Finished chunk 13, found 94479743 particles.\n",
      "[Worker 20443] Processing chunk 26...\n",
      "[Worker 20445] Finished chunk 11, found 93974250 particles.\n",
      "[Worker 20446] Processing chunk 28...\n",
      "[Worker 20445] Processing chunk 30...\n",
      "[Worker 20447] Finished chunk 16, found 94093496 particles.\n",
      "[Worker 20447] Processing chunk 17...\n",
      "[Worker 20440] Finished chunk 18, found 94035442 particles.\n",
      "[Worker 20440] Processing chunk 19...\n",
      "[Worker 20441] Finished chunk 20, found 93928979 particles.\n",
      "[Worker 20441] Processing chunk 21...\n",
      "[Worker 20442] Finished chunk 22, found 94715385 particles.\n",
      "[Worker 20442] Processing chunk 23...\n",
      "[Worker 20444] Finished chunk 24, found 93809796 particles.\n",
      "[Worker 20444] Processing chunk 25...\n",
      "[Worker 20443] Finished chunk 26, found 94268476 particles.\n",
      "[Worker 20443] Processing chunk 27...\n",
      "[Worker 20445] Finished chunk 30, found 93964049 particles.\n",
      "[Worker 20445] Processing chunk 31...\n",
      "[Worker 20446] Finished chunk 28, found 94168830 particles.\n",
      "[Worker 20446] Processing chunk 29...\n",
      "[Worker 20447] Finished chunk 17, found 93987608 particles.\n",
      "[Worker 20447] Processing chunk 32...\n",
      "[Worker 20440] Finished chunk 19, found 94429058 particles.\n",
      "[Worker 20440] Processing chunk 34...\n",
      "[Worker 20441] Finished chunk 21, found 94244066 particles.\n",
      "[Worker 20441] Processing chunk 36...\n",
      "[Worker 20442] Finished chunk 23, found 94032616 particles.\n",
      "[Worker 20442] Processing chunk 38...\n",
      "[Worker 20444] Finished chunk 25, found 94144667 particles.\n",
      "[Worker 20444] Processing chunk 40...\n",
      "[Worker 20443] Finished chunk 27, found 94481730 particles.\n",
      "[Worker 20443] Processing chunk 42...\n",
      "[Worker 20445] Finished chunk 31, found 93821773 particles.\n",
      "[Worker 20446] Finished chunk 29, found 94060527 particles.\n",
      "[Worker 20445] Processing chunk 44...\n",
      "[Worker 20446] Processing chunk 46...\n",
      "[Worker 20447] Finished chunk 32, found 93660669 particles.\n",
      "[Worker 20447] Processing chunk 33...\n",
      "[Worker 20440] Finished chunk 34, found 94253681 particles.\n",
      "[Worker 20440] Processing chunk 35...\n",
      "[Worker 20441] Finished chunk 36, found 94903597 particles.\n",
      "[Worker 20441] Processing chunk 37...\n",
      "[Worker 20442] Finished chunk 38, found 93949549 particles.\n",
      "[Worker 20442] Processing chunk 39...\n",
      "[Worker 20444] Finished chunk 40, found 94089610 particles.\n",
      "[Worker 20444] Processing chunk 41...\n",
      "[Worker 20443] Finished chunk 42, found 94282488 particles.\n",
      "[Worker 20443] Processing chunk 43...\n",
      "[Worker 20445] Finished chunk 44, found 94185208 particles.\n",
      "[Worker 20445] Processing chunk 45...\n",
      "[Worker 20446] Finished chunk 46, found 94460845 particles.\n",
      "[Worker 20446] Processing chunk 47...\n",
      "[Worker 20447] Finished chunk 33, found 94120680 particles.\n",
      "[Worker 20447] Processing chunk 48...\n",
      "[Worker 20440] Finished chunk 35, found 94391070 particles.\n",
      "[Worker 20440] Processing chunk 50...\n",
      "[Worker 20441] Finished chunk 37, found 94599589 particles.\n",
      "[Worker 20441] Processing chunk 52...\n",
      "[Worker 20442] Finished chunk 39, found 94196487 particles.\n",
      "[Worker 20442] Processing chunk 54...\n",
      "[Worker 20444] Finished chunk 41, found 94243475 particles.\n",
      "[Worker 20444] Processing chunk 56...\n",
      "[Worker 20443] Finished chunk 43, found 93935143 particles.\n",
      "[Worker 20443] Processing chunk 58...\n",
      "[Worker 20446] Finished chunk 47, found 93981043 particles.\n",
      "[Worker 20446] Processing chunk 60...\n",
      "[Worker 20445] Finished chunk 45, found 94651271 particles.\n",
      "[Worker 20445] Processing chunk 62...\n",
      "[Worker 20447] Finished chunk 48, found 94896095 particles.\n",
      "[Worker 20447] Processing chunk 49...\n",
      "[Worker 20440] Finished chunk 50, found 94622084 particles.\n",
      "[Worker 20440] Processing chunk 51...\n",
      "[Worker 20441] Finished chunk 52, found 94181796 particles.\n",
      "[Worker 20441] Processing chunk 53...\n",
      "[Worker 20442] Finished chunk 54, found 94192262 particles.\n",
      "[Worker 20442] Processing chunk 55...\n",
      "[Worker 20444] Finished chunk 56, found 94112448 particles.\n",
      "[Worker 20444] Processing chunk 57...\n",
      "[Worker 20443] Finished chunk 58, found 93807644 particles.\n",
      "[Worker 20443] Processing chunk 59...\n",
      "[Worker 20445] Finished chunk 62, found 94148871 particles.\n",
      "[Worker 20445] Processing chunk 63...\n",
      "[Worker 20446] Finished chunk 60, found 94843907 particles.\n",
      "[Worker 20446] Processing chunk 61...\n",
      "[Worker 20447] Finished chunk 49, found 94205856 particles.\n",
      "[Worker 20440] Finished chunk 51, found 94390029 particles.\n",
      "[Worker 20441] Finished chunk 53, found 94219272 particles.\n",
      "[Worker 20442] Finished chunk 55, found 94238802 particles.\n",
      "[Worker 20444] Finished chunk 57, found 94647825 particles.\n",
      "[Worker 20443] Finished chunk 59, found 94416436 particles.\n",
      "[Worker 20445] Finished chunk 63, found 94062469 particles.\n",
      "[Worker 20446] Finished chunk 61, found 93726053 particles.\n",
      "\n",
      "Parallel processing finished. Aggregating results...\n",
      "Total particles aggregated: 6028568000\n",
      "--- Density field generation took 3225.56 seconds ---\n",
      "Calculating tidal field via FFT...\n",
      "Inverse transforming back to real space...\n",
      "Calculating eigenvalues...\n",
      "Saving results to /data14/Fhong/halo_web/field_sample_256_hr.hdf5...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Generate Field Samples\n",
    "\n",
    "snapshot_dir = BASE_PATH / 'output'\n",
    "sim_info_file = OUTPUT_PATH / 'tng100-1-dark.json'\n",
    "snapshot_num = 99\n",
    "\n",
    "# T-Web Algorithm Parameters\n",
    "grid_size = 256  # The number of grid cells along one dimension\n",
    "smoothing_scale = 0.3  # Gaussian smoothing scale in Mpc/h, as used in theory.ipynb\n",
    "lambda_th = 0.2  # Eigenvalue threshold for web classification, as used in theory.ipynb\n",
    "\n",
    "# Output file\n",
    "output_file = OUTPUT_PATH / f'field_sample_{grid_size}_hr.hdf5'\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "\n",
    "def get_para(snap_dir: Path, snap_num: int) -> dict:\n",
    "    first_file_path = snap_dir / f'snap_{snap_num:03d}.0.hdf5'\n",
    "    print(f\"Reading header from: {first_file_path}\")\n",
    "    with h5py.File(first_file_path, 'r', locking=False) as f:\n",
    "        header = f['Header'].attrs\n",
    "        box_size_ckpc_h = header['BoxSize']      # In ckpc/h \n",
    "        num_files = header['NumFilesPerSnapshot'] # Number of chunks \n",
    "        \n",
    "    return {\n",
    "        'box_size_mpc_h': box_size_ckpc_h / 1000.0,\n",
    "        'num_files': num_files\n",
    "    }\n",
    "\n",
    "def cic_deposit_chunk(positions: np.ndarray, box_size: float, n_grid: int, target_field: np.ndarray):\n",
    "    pos_grid = positions / box_size * n_grid\n",
    "    \n",
    "    i = np.floor(pos_grid).astype(int)\n",
    "    f = pos_grid - i\n",
    "    \n",
    "    w = [(1 - f, f), (1 - f, f), (1 - f, f)]\n",
    "    \n",
    "    for dx in [0, 1]:\n",
    "        for dy in [0, 1]:\n",
    "            for dz in [0, 1]:\n",
    "                weight = w[0][dx][:, 0] * w[1][dy][:, 1] * w[2][dz][:, 2]\n",
    "                indices = ( (i[:, 0] + dx) % n_grid, \n",
    "                            (i[:, 1] + dy) % n_grid, \n",
    "                            (i[:, 2] + dz) % n_grid )\n",
    "                np.add.at(target_field, indices, weight)\n",
    "\n",
    "def process_chunk(chunk_index: int, snap_dir: Path, snap_num: int, box_size: float, n_grid: int) -> tuple[np.ndarray, int]:\n",
    "    file_path = snap_dir / f'snap_{snap_num:03d}.{chunk_index}.hdf5'\n",
    "    print(f\"[Worker {os.getpid()}] Processing chunk {chunk_index}...\")\n",
    "\n",
    "    partial_density_field = np.zeros((n_grid, n_grid, n_grid), dtype=np.float32)\n",
    "    num_particles_in_chunk = 0\n",
    "\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r', locking = False) as f:\n",
    "            if 'PartType1' in f and 'Coordinates' in f['PartType1']:\n",
    "                positions_chunk = f['PartType1']['Coordinates'][:].astype(np.float32) / 1000.0\n",
    "                num_particles_in_chunk = positions_chunk.shape[0]\n",
    "                cic_deposit_chunk(positions_chunk, box_size, n_grid, partial_density_field)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  [Worker {os.getpid()}] WARNING: File {file_path} not found. Skipping.\")\n",
    "\n",
    "    print(f\"[Worker {os.getpid()}] Finished chunk {chunk_index}, found {num_particles_in_chunk} particles.\")\n",
    "    return partial_density_field, num_particles_in_chunk\n",
    "\n",
    "\n",
    "snapshot_dir = BASE_PATH / f'output/snapdir_{snapshot_num:03d}'\n",
    "    \n",
    "sim_params = get_para(snapshot_dir, snapshot_num)\n",
    "box_size = sim_params['box_size_mpc_h']\n",
    "num_files = sim_params['num_files'] \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "task_func = partial(process_chunk, \n",
    "                        snap_dir=snapshot_dir, \n",
    "                        snap_num=snapshot_num, \n",
    "                        box_size=box_size, \n",
    "                        n_grid=grid_size)\n",
    "    \n",
    "chunk_indices = range(num_files)\n",
    "\n",
    "print(f\"\\nStarting parallel density field calculation with {num_processes} processes...\")\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    results = pool.map(task_func, chunk_indices)\n",
    "\n",
    "\n",
    "print(\"\\nParallel processing finished. Aggregating results...\")\n",
    "partial_fields = [res[0] for res in results]\n",
    "particle_counts = [res[1] for res in results]\n",
    "\n",
    "density_field = np.sum(partial_fields, axis=0, dtype=np.float32)\n",
    "total_particles = np.sum(particle_counts)\n",
    "del partial_fields, results \n",
    "\n",
    "print(f\"Total particles aggregated: {total_particles}\")\n",
    "time_after_cic = time.time()\n",
    "\n",
    "\n",
    "print(f\"--- Density field generation took {time_after_cic - start_time:.2f} seconds ---\")\n",
    "mean_density = total_particles / grid_size**3\n",
    "delta_field = density_field / mean_density - 1.0\n",
    "del density_field\n",
    "\n",
    "print(\"Calculating tidal field via FFT...\")\n",
    "delta_k = rfftn(delta_field)\n",
    "\n",
    "k_values = 2 * np.pi * np.fft.fftfreq(grid_size, d=box_size / grid_size)\n",
    "kx, ky, kz_rfft = np.meshgrid(k_values, k_values, k_values[:grid_size//2 + 1], indexing='ij', sparse=True)\n",
    "\n",
    "k_sq = kx**2 + ky**2 + kz_rfft**2\n",
    "k_sq[0, 0, 0] = 1.0\n",
    "\n",
    "smoothing_factor_k = np.exp(-0.5 * k_sq * smoothing_scale**2)\n",
    "delta_sm_k = delta_k * smoothing_factor_k\n",
    "\n",
    "T_ij_k = {}\n",
    "T_ij_k['11'] = (kx * kx / k_sq - 1/3) * delta_sm_k\n",
    "T_ij_k['12'] = (kx * ky / k_sq)       * delta_sm_k\n",
    "T_ij_k['13'] = (kx * kz_rfft / k_sq)  * delta_sm_k\n",
    "T_ij_k['22'] = (ky * ky / k_sq - 1/3) * delta_sm_k\n",
    "T_ij_k['23'] = (ky * kz_rfft / k_sq)  * delta_sm_k\n",
    "T_ij_k['33'] = (kz_rfft * kz_rfft / k_sq - 1/3) * delta_sm_k\n",
    "\n",
    "print(\"Inverse transforming back to real space...\")\n",
    "T_ij = {key: irfftn(val, s=(grid_size,)*3) for key, val in T_ij_k.items()}\n",
    "delta_sm_x = irfftn(delta_sm_k, s=(grid_size,)*3)\n",
    "\n",
    "print(\"Calculating eigenvalues...\")\n",
    "tidal_tensor = np.array([\n",
    "    [T_ij['11'], T_ij['12'], T_ij['13']],\n",
    "    [T_ij['12'], T_ij['22'], T_ij['23']],\n",
    "    [T_ij['13'], T_ij['23'], T_ij['33']]\n",
    "])\n",
    "tidal_tensor = np.moveaxis(tidal_tensor, [0, 1], [-2, -1])\n",
    "eigenvalues = np.linalg.eigh(tidal_tensor)[0][..., ::-1]\n",
    "\n",
    "print(f\"Saving results to {output_file}...\")\n",
    "with h5py.File(output_file, 'w', locking = False) as f:\n",
    "    f.create_dataset('lams', data=eigenvalues.astype(np.float32))\n",
    "    f.create_dataset('delta_sm_x', data=delta_sm_x.astype(np.float32))\n",
    "    f.create_dataset('l_box', data=box_size)\n",
    "    f.create_dataset('n_grids', data=grid_size)\n",
    "    n_above_thresh = (eigenvalues > lambda_th).sum(axis=3)\n",
    "    f.create_dataset('web_type', data=n_above_thresh.astype(np.int8))\n",
    "    f.attrs['smoothing_scale_mpc_h'] = smoothing_scale\n",
    "    f.attrs['lambda_th'] = lambda_th\n",
    "    \n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
